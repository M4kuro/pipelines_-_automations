# Automated Data Ingestion & Reporting Pipeline

## ğŸ“Œ Project Overview

This project demonstrates a **fully automated data pipeline** built with Python. The pipeline ingests raw CSV files, cleans and standardizes the data, and produces processed datasets and summary reports â€” all executed with a single command.

The focus of this project is **automation, reproducibility, and clean data workflows**, rather than dashboards or manual analysis.

---

## ğŸ¯ Objectives

* Automate ingestion of multiple raw CSV files
* Apply consistent data cleaning and validation rules
* Generate processed datasets ready for analysis
* Automatically produce summary reports
* Demonstrate end-to-end pipeline structure (Extract â†’ Transform â†’ Load)

---

## ğŸ§± Project Structure

```
project-2-automation-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # Input CSV files
â”‚   â””â”€â”€ processed/     # Cleaned output data
â”‚
â”œâ”€â”€ reports/            # Automatically generated reports
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py     # Data ingestion logic
â”‚   â”œâ”€â”€ transform.py   # Cleaning & validation logic
â”‚   â”œâ”€â”€ load.py        # Output & reporting logic
â”‚   â””â”€â”€ main.py        # Pipeline orchestrator
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Steps

### 1ï¸âƒ£ Extract

* Automatically reads **all CSV files** from `data/raw/`
* Combines them into a single dataset
* Tracks source file names for traceability

### 2ï¸âƒ£ Transform

* Standardizes column names
* Removes duplicate records
* Handles missing values
* Ensures consistent schema

### 3ï¸âƒ£ Load

* Saves cleaned data to `data/processed/cleaned_data.csv`
* Generates a statistical summary report in `reports/summary_report.csv`

---

## â–¶ï¸ How to Run

### 1. Create virtual environment (optional)

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Add raw data

Place one or more CSV files into:

```
data/raw/
```

### 4. Run the pipeline

From the **project root**:

```bash
python -m src.main
```

---

## Outputs

After execution, the pipeline generates:

* `data/processed/cleaned_data.csv`
* `reports/summary_report.csv`

All outputs are created automatically.

---

##  Key Skills Demonstrated

* Python automation
* Modular ETL pipeline design
* Data cleaning & validation
* File system orchestration
* Reproducible workflows

---

##  Next Improvements (Planned)

* API-based ingestion
* Database loading (SQLite / PostgreSQL)
* Logging & error handling
* Scheduling (cron / Task Scheduler)

---


